{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accurate-building",
   "metadata": {},
   "source": [
    "# Modele de réseau neuronale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-hometown",
   "metadata": {},
   "source": [
    "Toutes les étapes pour arriver à un modele correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-miami",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "diverse-fraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "#import seaborn as sns #On ne sait jamais que ça serve\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler #Normalisation #Il faut l'installer aussi avec conda install\n",
    "import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.layers import  Dense, Conv3D, BatchNormalization,MaxPooling3D, Dropout, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#conda install pydot\n",
    "#conda install pydotplus\n",
    "#conda install graphviz\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#import pydot\n",
    "\n",
    "\n",
    "import threading\n",
    "from queue import Queue, Empty\n",
    "from time import sleep, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-arkansas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "healthy-studio",
   "metadata": {},
   "source": [
    "### Paramètres généraux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indoor-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10 #Nombre de classes\n",
    "fps = 8\n",
    "size = (40,30)\n",
    "#Parametres du generator\n",
    "pack_size = 50\n",
    "batch_size = 25\n",
    "epochs = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "criminal-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved_model\\model_LSTM_8_30_40_10_10_20_50_500mili\n"
     ]
    }
   ],
   "source": [
    "#Verification qu'un modele avec ces paramètres n'existe pas déjà\n",
    "folder = 'Saved_model'\n",
    "modele_type = 'model_LSTM'\n",
    "param = f\"_{fps}_{size[1]}_{size[0]}_{nb_classes}_{epochs}_{batch_size}_{pack_size}_{int(learning_rate*1000)}mili\"\n",
    "full_name = folder + '\\\\' + modele_type + param\n",
    "print(full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "advance-maria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modele non testé\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_saved = keras.models.load_model(full_name)\n",
    "    print(\"Pas besoin d'aller plus loin, le modele a déjà été fabriqué\")\n",
    "    model_charge = True\n",
    "except:\n",
    "    print(\"Modele non testé\")\n",
    "    model_charge = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-train",
   "metadata": {},
   "source": [
    "## Fonctions d'imports et preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-scene",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wooden-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mov_imgs_from_path(path,fps = -1,color = 'gray'):\n",
    "    '''\n",
    "    Retourne une liste d'images pour une vidéo. La liste d'image a le nombre de fps voulu\n",
    "    reprend que le mouvement\n",
    "    '''\n",
    "    \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    fps_actu = cap.get(cv2.CAP_PROP_FPS)\n",
    "#     print(f\"Traitement de la video n° {path}       \",end=\"\\r\")\n",
    "    if fps <= -1: fps = fps_actu #Je peux ne pas donner de fps et ça va prendre le nombre d'fps initial\n",
    "    ecart_voulu = int(1000/fps)\n",
    "    ecart_initial = int(1000/fps_actu)\n",
    "    imgs = []\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    while(cap.isOpened()):\n",
    "        prev = frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret: #Sinon ça plante quand il n'y a plus d'images\n",
    "            #Récupère seulement certaines images\n",
    "            t_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "            modulo = t_ms % ecart_voulu\n",
    "            if modulo < ecart_initial:\n",
    "                #Isoler les images\n",
    "                \n",
    "                diff = cv2.absdiff(frame,prev)  \n",
    "                if color == 'gray':\n",
    "                    diff_gray = cv2.cvtColor(diff,cv2.COLOR_BGR2GRAY)\n",
    "                elif color == 'rgb':\n",
    "                    diff_gray = diff\n",
    "\n",
    "                    \n",
    "                #Première normalisation (elle n'est peut-être pas obligatoire)\n",
    "                max_ = np.max(diff_gray)\n",
    "                if max_ == 0:\n",
    "                    max_ = 1\n",
    "                ratio = 255.0 / max_\n",
    "                diff_gray = diff_gray * ratio\n",
    "                \n",
    "                \n",
    "                #On va travailler avec des images en nuance de gris, c'est bcp plus simple\n",
    "                imgs.append(diff_gray)\n",
    "            \n",
    "        else: #Va jusqu'au bout de la vidéo\n",
    "            break\n",
    "    else:\n",
    "        print(\"Le fichier n'a pas pu être ouvert\")\n",
    "    cap.release()\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-prefix",
   "metadata": {},
   "source": [
    "#### Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "korean-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_imgs(imgs,nsize):\n",
    "    '''\n",
    "    Change la taille de l'image.\n",
    "    Le premier élément de nsize est la longueur (width), le deuxième la hauteur (height)\n",
    "    '''\n",
    "    return [cv2.resize(img, dsize=nsize, interpolation=cv2.INTER_CUBIC) for img in imgs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-palace",
   "metadata": {},
   "source": [
    "#### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "behind-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_by_pixels(img, x_min, x_max):\n",
    "    '''\n",
    "    Scale une image entre 0 et 1.\n",
    "    Méthode pas efficace mais fonctionnelle contrairement à d'autres méthodes \n",
    "    qui faisaient lignes par lignes ce qui causait des erreurs \n",
    "    dans le résultat (lignes dans l'image)\n",
    "    '''\n",
    "    new_img = img.copy()\n",
    "    p_min = 1000\n",
    "    p_max = -1000\n",
    "    for line in img:\n",
    "        for pixel in line:\n",
    "            p_min = min(p_min,pixel)\n",
    "            p_max = max(p_max,pixel)\n",
    "    \n",
    "    #p_min et p_max sont les min et max totaux de mon image\n",
    "    for l, line in enumerate(img):\n",
    "        for p,pixel in enumerate(line):\n",
    "            nom = (pixel - p_min)*(x_max - x_min)\n",
    "            denom = (p_max - p_min)\n",
    "            if denom == 0: denom = 1\n",
    "            new_img[l][p] = x_min + nom/denom\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qualified-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_imgs(imgs):\n",
    "    '''\n",
    "    Normalise une série d'images\n",
    "    '''\n",
    "    norm_imgs = []\n",
    "    for img in imgs:\n",
    "        norm_img = scale_by_pixels(img,0,1)\n",
    "        norm_imgs.append(norm_img)\n",
    "#     print(f\"Après normalisation, max = {np.max(norm_imgs)} et min = {np.min(norm_imgs)}\")\n",
    "    return norm_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "interested-nursery",
   "metadata": {},
   "source": [
    "## Fonction de générateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reduced-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_gen(labels_list, folder_path, pack_size = 10, pointer = 0, fps = 5, size = (160,120)):\n",
    "    '''\n",
    "    à partir de la liste des labels et de la position du foler\n",
    "    Retourne:\n",
    "    -une liste d'images de vidéos de shape (pack_size,n_frames,height,width[,channels])\n",
    "        n_frames = fps*2.4\n",
    "    -Une liste de numérique avec les labels correspondant aux images (de len = batch_size)\n",
    "    -Un pointer qui permet de faire tourner my_gen à nouveau et recevoir les éléments suivants de la liste\n",
    "    \n",
    "    \n",
    "    Normalement on peut envoyer X et y_num dans le modele directement (avec fit ou train_on_batch, les deux devraient fonctionner)\n",
    "    Je vais essayer de travailler avec des thread pour avoir un thread en Train et un trhead en gen\n",
    "    '''\n",
    "    \n",
    "    y_num = []\n",
    "    X = []\n",
    "    for i in range(pointer, pack_size + pointer):\n",
    "        pointer = i+1\n",
    "        if i >= len(labels_list):\n",
    "            break\n",
    "        label, video_name = labels_list[i]\n",
    "        \n",
    "        #Label en numérique\n",
    "        y_num.append(labels_n[label])\n",
    "        \n",
    "        #Preprocess d'images\n",
    "        file_path = folder_path + \"\\\\\" + video_name\n",
    "        imgs_of_video = get_mov_imgs_from_path(file_path,fps,'gray') #Gray ou rgb\n",
    "        resized_imgs = resize_imgs(imgs_of_video, size)\n",
    "        norm_imgs = normalize_imgs(resized_imgs)\n",
    "        \n",
    "        X.append(norm_imgs)\n",
    "        \n",
    "    print() #Pour ne pas écrire sur la même ligne qu'avant\n",
    "    X = np.array(X)\n",
    "    y = np.array(y_num)\n",
    "    \n",
    "    #Ajouter une dimsension en à la fin est nécessaire pour certains modèle\n",
    "    #Commenter cette ligne si ce n'est pas nécessaire\n",
    "    X = np.expand_dims(X, axis=len(X.shape)) \n",
    "    \n",
    "    # Si le modele est compile avec :loss='sparse_categorical_crossentropy'\n",
    "    # Il faut un y catégorique , faire ça hors de la fonction\n",
    "    #y = to_categorical(y, num_classes = nb_classes)\n",
    "\n",
    "    return (X, y, pointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "undefined-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_thread(labels_list, folder_path, pack_size = 10, fps = 5, size = (160,120)):\n",
    "    '''\n",
    "    S'assure que my_gen tourne toujours en parallèle du fit. Il prépare toujours 2 paquets de vidéo en avance\n",
    "    '''\n",
    "    pointer = 0\n",
    "    while pointer < len(labels_list):\n",
    "        X,y,pointer = my_gen(labels_list, folder_path, pack_size, pointer, fps, size)\n",
    "        q.put((X,y))\n",
    "        print(f\"Pack of {pack_size} videos put in queue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-event",
   "metadata": {},
   "source": [
    "## Fabrication des modeles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-session",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-settle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "based-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_frames = 24, height = 120, width = 160, channels = 1, nb_classes = 10):\n",
    "    #Gestion du nombre de frames\n",
    "    #entre 12 et 24 frames, ça donne entre 3 et 6 images au LSTM, c'est très bien. \n",
    "    div_pool1 = 2\n",
    "    div_pool2 = 2\n",
    "    if n_frames < 12:\n",
    "        div_pool1 = 1\n",
    "    elif n_frames >= 36:\n",
    "        div_pool1 = 3\n",
    "    if n_frames < 6:\n",
    "        div_pool2 = 1\n",
    "    \n",
    "    \n",
    "    #Dernière couche avant reshape\n",
    "    last_channel = 10\n",
    "    \n",
    "    #Gestion de la hauteur et la width:\n",
    "    output_shape = int(height/27) * int(width/27) * last_channel\n",
    "    \n",
    "    #Divisé par 27 parce qu'on divise par 3, 3 fois (avec Maxpooling)\n",
    "\n",
    "    \n",
    "    sample_shape = (n_frames,height,width,channels) #width = 160, height = 120, nframes = 24, 3 channels si on est en RGB (si on est en gris on sait pas)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #model.add(Conv3D(128, strides =(1,1,1), padding=\"same\", kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=sample_shape,data_format='channels_first'))\n",
    "    model.add(Conv3D(192, strides =(1,1,1), padding=\"same\", kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=sample_shape,data_format='channels_last'))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Conv3D(96, strides=(1,1,1), padding=\"same\", kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(MaxPooling3D(pool_size=(div_pool1, 3, 3)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv3D(96, strides=(1,1,1), padding=\"same\", kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(96, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Conv3D(64, strides=(1,1,1), padding=\"same\", kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(MaxPooling3D(pool_size=(div_pool2, 3, 3)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv3D(48, strides=(1,1,1), padding=\"same\", kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(48, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Conv3D(32, strides=(1,1,1), padding=\"same\", kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(MaxPooling3D(pool_size=(3, 3, 3), data_format = 'channels_first')) #Je ne veux pas diviser le nombre d'images cette fois-ci\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv3D(last_channel, strides=(1,1,1), padding=\"same\", kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(last_channel, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "\n",
    "\n",
    "    model.add(layers.Reshape((int(n_frames/(div_pool1 * div_pool2)),output_shape)))\n",
    "\n",
    "\n",
    "    #LSTM\n",
    "    \n",
    "    model.add(LSTM(units = 64,activation=\"tanh\", recurrent_activation=\"sigmoid\",return_sequences = False))\n",
    "\n",
    "\n",
    "    #softmax\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "specified-hydrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pas de modele chargé, il faut en créer un nouveau\n"
     ]
    }
   ],
   "source": [
    "n_frames = int(fps*2.4)\n",
    "\n",
    "if model_charge:\n",
    "    print('Modele chargé')\n",
    "    model = model_saved\n",
    "else:\n",
    "    print(\"Pas de modele chargé, il faut en créer un nouveau\")\n",
    "    model = create_model(n_frames = n_frames, height = size[1], width = size[0], nb_classes = nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "great-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "available-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_name = full_name + '_plot.png'\n",
    "# plot_model(model, to_file = plot_name, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "directed-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compilation\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "              loss = tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-setup",
   "metadata": {},
   "source": [
    "##### Chargement d'un modele préentrainé (par nous même)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "lesbian-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_stolen_saved = keras.models.load_model('Saved_model\\\\modele_stolen_compile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-filter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ignored-arnold",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "infinite-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homemade_fit_gen_thread(model, labels_val,pack_size,epochs,batch_size,fps = 5,size = (160,120)):\n",
    "    '''\n",
    "    Charge les données à partir du csv labels_val et train le model avec\n",
    "    les différents paramètres\n",
    "    Le model est train.\n",
    "    \n",
    "    Si besoin, return de X,y et model (mais normalement le model original est train)\n",
    "    '''\n",
    "#     q = Queue()\n",
    "    \n",
    "    histories = []\n",
    "    \n",
    "    t = threading.Thread(target = gen_thread, daemon = True,#Daemon afin que le thread s'arrête avec le thread principal\n",
    "                        args = ((labels_val, \"DATA\\\\Videos\",pack_size,fps, size))) \n",
    "    t.start()\n",
    "    while True:\n",
    "        X, y_num = q.get()\n",
    "        print(\"I got the pack! I'm going to fit it\")\n",
    "        \n",
    "        #Catégorisation pour train\n",
    "        y = to_categorical(y_num, num_classes = nb_classes)\n",
    "        \n",
    "        history = model.fit(X, y,\n",
    "                batch_size = 10,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_split=0.2)\n",
    "        histories.append(history)\n",
    "        \n",
    "        q.task_done() #Jsp si c'est utile\n",
    "        \n",
    "        #Il faut que le fit prenne plus de temps que le chargement des vidéos, sinon ça va pas aller\n",
    "        \n",
    "        if q.empty():\n",
    "            print(\"J'ai peut-être fini, j'attends encore qq sec pour être sur d'avoir bien fini\")\n",
    "            sleep(60) #Je m'assure que c'est vraiment bien fini\n",
    "            if q.empty():\n",
    "                print(\"J'ai vraiment fini\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Je n'avais pas fini en fait\")\n",
    "    \n",
    "    t.join()\n",
    "    return model, histories, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "reported-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homemade_fit_gen(model, labels_val,pack_size,epochs,batch_size,fps = 5,size = (160,120)):\n",
    "    '''\n",
    "    Charge les données à partir du csv labels_val et train le model avec\n",
    "    les différents paramètres\n",
    "    Le model est train.\n",
    "    \n",
    "    my_gen et fit tournent en parallèle grâce à un thread\n",
    "    Si besoin, return de X,y et model (mais normalement le model original est train)\n",
    "    '''\n",
    "    \n",
    "    histories = []\n",
    "    \n",
    "    pointer = 0\n",
    "    can_continue = True\n",
    "    \n",
    "    while can_continue:\n",
    "        X, y_num, pointer = my_gen(labels_val, \"DATA\\\\Videos\",pack_size = pack_size,fps = fps, size = size, pointer = pointer)\n",
    "        \n",
    "        #Catégorisation pour train\n",
    "        y = to_categorical(y_num, num_classes = nb_classes)\n",
    "        \n",
    "        if pointer >= len(labels_val):\n",
    "            can_continue = False\n",
    "        print()\n",
    "\n",
    "        #print('vidéos chargées, shape:',X.shape,'. y len : ',len(y_num)) #DEBUG\n",
    "        with tf.device('/gpu:1'):\n",
    "            history = model.fit(X, y,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "        del X\n",
    "        del y\n",
    "        del y_num\n",
    "        histories.append(history)\n",
    "    return model, histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-night",
   "metadata": {},
   "source": [
    "## Import des données et traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "falling-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des labels\n",
    "labels_csv = pd.read_csv(\"DATA\\\\labels.csv\")\n",
    "labels_name = pd.read_csv(\"DATA\\\\labels_uses.csv\")\n",
    "labels_tests_csv = pd.read_csv(\"DATA\\\\labels_tests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "delayed-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Créé un dictionnaire qui associe le nom du label et un numérique afin de le mettre dans le modele\n",
    "labels_n = {}\n",
    "for i,label in enumerate(labels_name.values):\n",
    "    labels_n[label[0]] = i\n",
    "    labels_n[i] = label[0]\n",
    "\n",
    "    \n",
    "#Trier les labels pour ne prendre que ceux qui correspondent à labels_uses\n",
    "labels = []\n",
    "for label, video_name in labels_csv.values:\n",
    "    if label in labels_n:\n",
    "        labels.append((label,video_name))\n",
    "        \n",
    "#Idem pour le label de tests\n",
    "labels_tests = []\n",
    "for label, video_name in labels_tests_csv.values:\n",
    "    if label in labels_n:\n",
    "        labels_tests.append((label,video_name))\n",
    "\n",
    "\n",
    "#Melange la liste de train\n",
    "random.shuffle(labels) #Il faut absolument shuffle pour train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "civil-share",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ferme le poing': 53, 'Ouvre le poing': 56, 'Clap': 51, 'Swipe gauche main droite': 45, 'Swipe droit main gauche': 46, 'Doigts croises paumes': 47, 'Explosion avec les doigts ouverts': 49, 'Ouvrir un livre avec le poing ferme main D': 40, 'Ouvrir un livre avec le poing ferme main G': 35, 'Faire coucou': 62}\n"
     ]
    }
   ],
   "source": [
    "#Count de chaque label\n",
    "labels_c = {}\n",
    "for label in labels_name.values:\n",
    "    labels_c[label[0]] = 0\n",
    "\n",
    "for label,video_name in labels:\n",
    "    if label in labels_c:\n",
    "        labels_c[label] += 1\n",
    "\n",
    "print(labels_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-programming",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "promising-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Queue(maxsize = 3) #Variable globale nécessaire pour le bon fonctionnement du gen en thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "graduate-traffic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il faut entrainer le modele\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.7656 - accuracy: 0.0750 - val_loss: 2.9150 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.9645 - accuracy: 0.1000 - val_loss: 2.3638 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.8401 - accuracy: 0.1250 - val_loss: 3.4375 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1360 - accuracy: 0.1000 - val_loss: 3.5639 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.7781 - accuracy: 0.0500 - val_loss: 2.6010 - val_accuracy: 0.2000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 4.0206 - accuracy: 0.0750 - val_loss: 3.6971 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1909 - accuracy: 0.1250 - val_loss: 2.2891 - val_accuracy: 0.2000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1678 - accuracy: 0.0500 - val_loss: 3.8834 - val_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.7495 - accuracy: 0.0750 - val_loss: 2.6001 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.9642 - accuracy: 0.1500 - val_loss: 2.4881 - val_accuracy: 0.2000\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.8834 - accuracy: 0.0750 - val_loss: 1.9807 - val_accuracy: 0.2000\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.8993 - accuracy: 0.1750 - val_loss: 2.9800 - val_accuracy: 0.2000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.9752 - accuracy: 0.1500 - val_loss: 3.7478 - val_accuracy: 0.1000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.8540 - accuracy: 0.0250 - val_loss: 2.3429 - val_accuracy: 0.2000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.7711 - accuracy: 0.0750 - val_loss: 3.6227 - val_accuracy: 0.1000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1215 - accuracy: 0.1500 - val_loss: 2.4303 - val_accuracy: 0.1000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1174 - accuracy: 0.1250 - val_loss: 2.9673 - val_accuracy: 0.1000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.7763 - accuracy: 0.1000 - val_loss: 3.4662 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 4.3769 - accuracy: 0.0750 - val_loss: 3.0962 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.5770 - accuracy: 0.1500 - val_loss: 3.8093 - val_accuracy: 0.2000\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 5.0371 - accuracy: 0.0750 - val_loss: 3.2254 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 4.6788 - accuracy: 0.1250 - val_loss: 2.9898 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.9925 - accuracy: 0.1250 - val_loss: 4.0591 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.8481 - accuracy: 0.1000 - val_loss: 4.5966 - val_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.2001 - accuracy: 0.1500 - val_loss: 2.5867 - val_accuracy: 0.3000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.8018 - accuracy: 0.1250 - val_loss: 3.5804 - val_accuracy: 0.1000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.9884 - accuracy: 0.0750 - val_loss: 3.6739 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.7151 - accuracy: 0.0750 - val_loss: 3.5609 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1990 - accuracy: 0.1500 - val_loss: 2.9611 - val_accuracy: 0.3000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.8972 - accuracy: 0.1000 - val_loss: 4.2119 - val_accuracy: 0.0000e+00\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3988 - accuracy: 0.0750 - val_loss: 5.2072 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.6757 - accuracy: 0.0750 - val_loss: 3.8664 - val_accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.4180 - accuracy: 0.1750 - val_loss: 3.2206 - val_accuracy: 0.2000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.4522 - accuracy: 0.1500 - val_loss: 2.7512 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.2943 - accuracy: 0.1250 - val_loss: 3.8267 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.0417 - accuracy: 0.1000 - val_loss: 3.1829 - val_accuracy: 0.1000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.8322 - accuracy: 0.0750 - val_loss: 3.1404 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.6052 - accuracy: 0.1000 - val_loss: 3.8609 - val_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 4.2263 - accuracy: 0.0000e+00 - val_loss: 3.3258 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.6377 - accuracy: 0.0750 - val_loss: 3.7778 - val_accuracy: 0.1000\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 4.0034 - accuracy: 0.1500 - val_loss: 4.8522 - val_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.6630 - accuracy: 0.1000 - val_loss: 2.2770 - val_accuracy: 0.4000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.5876 - accuracy: 0.1500 - val_loss: 3.7759 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3217 - accuracy: 0.0750 - val_loss: 2.3262 - val_accuracy: 0.4000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1929 - accuracy: 0.0750 - val_loss: 2.4099 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.8443 - accuracy: 0.1250 - val_loss: 3.4550 - val_accuracy: 0.1000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1299 - accuracy: 0.1500 - val_loss: 3.1783 - val_accuracy: 0.1000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3361 - accuracy: 0.0500 - val_loss: 3.5572 - val_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.2675 - accuracy: 0.1750 - val_loss: 2.3774 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3713 - accuracy: 0.1250 - val_loss: 2.0416 - val_accuracy: 0.4000\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1806 - accuracy: 0.1500 - val_loss: 4.4671 - val_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1983 - accuracy: 0.1500 - val_loss: 3.5222 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3282 - accuracy: 0.1500 - val_loss: 3.3186 - val_accuracy: 0.1000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.8802 - accuracy: 0.2000 - val_loss: 3.4192 - val_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.2320 - accuracy: 0.0750 - val_loss: 2.8945 - val_accuracy: 0.1000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.8687 - accuracy: 0.1500 - val_loss: 3.0657 - val_accuracy: 0.1000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1014 - accuracy: 0.1250 - val_loss: 2.8876 - val_accuracy: 0.1000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1887 - accuracy: 0.1000 - val_loss: 3.7296 - val_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1849 - accuracy: 0.1500 - val_loss: 2.4441 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.9489 - accuracy: 0.0500 - val_loss: 3.6497 - val_accuracy: 0.1000\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3754 - accuracy: 0.0750 - val_loss: 2.8535 - val_accuracy: 0.4000\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.4540 - accuracy: 0.0750 - val_loss: 3.4102 - val_accuracy: 0.4000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3252 - accuracy: 0.1500 - val_loss: 3.9285 - val_accuracy: 0.1000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.0201 - accuracy: 0.1250 - val_loss: 4.4890 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.5434 - accuracy: 0.1500 - val_loss: 3.3564 - val_accuracy: 0.4000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.7863 - accuracy: 0.1000 - val_loss: 3.5085 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.5593 - accuracy: 0.1750 - val_loss: 3.7741 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3794 - accuracy: 0.0750 - val_loss: 2.4894 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3324 - accuracy: 0.0500 - val_loss: 2.2028 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.4497 - accuracy: 0.1250 - val_loss: 4.3595 - val_accuracy: 0.0000e+00\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.8155 - accuracy: 0.0500 - val_loss: 5.5705 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3195 - accuracy: 0.1250 - val_loss: 3.4063 - val_accuracy: 0.2000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3216 - accuracy: 0.1000 - val_loss: 3.1439 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.9086 - accuracy: 0.3000 - val_loss: 2.4429 - val_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.3181 - accuracy: 0.1000 - val_loss: 3.6213 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.2429 - accuracy: 0.2250 - val_loss: 3.0529 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.0981 - accuracy: 0.3000 - val_loss: 1.8501 - val_accuracy: 0.1000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.4262 - accuracy: 0.1250 - val_loss: 3.7770 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.9330 - accuracy: 0.2000 - val_loss: 3.4317 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.9156 - accuracy: 0.1000 - val_loss: 3.4802 - val_accuracy: 0.0000e+00\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.2095 - accuracy: 0.0250 - val_loss: 2.6192 - val_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.9836 - accuracy: 0.0750 - val_loss: 2.7748 - val_accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.9959 - accuracy: 0.0750 - val_loss: 2.4963 - val_accuracy: 0.1000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.6652 - accuracy: 0.0500 - val_loss: 2.6433 - val_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.2356 - accuracy: 0.1000 - val_loss: 2.9349 - val_accuracy: 0.1000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.0551 - accuracy: 0.1250 - val_loss: 2.7267 - val_accuracy: 0.2000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1617 - accuracy: 0.0500 - val_loss: 2.6166 - val_accuracy: 0.1000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.8472 - accuracy: 0.0750 - val_loss: 2.9626 - val_accuracy: 0.2000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1373 - accuracy: 0.0750 - val_loss: 2.7718 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.1662 - accuracy: 0.1250 - val_loss: 2.7980 - val_accuracy: 0.2000\n",
      "\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[10,128,19,30,40] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/sequential/max_pooling3d/MaxPool3D/MaxPool3DGrad (defined at <ipython-input-17-43dbc76b3aa7>:27) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_11835]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f3e1386b5a65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Il faut entrainer le modele'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhomemade_fit_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpack_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpack_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"ça a prit {end-start} secondes d'entrainer le modele\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-04f0997e0058>\u001b[0m in \u001b[0;36mhomemade_fit_gen\u001b[1;34m(model, labels_val, pack_size, epochs, batch_size, fps, size)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 validation_split=0.2)\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mhistories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\Users\\nicol\\anaconda3\\envs\\DL3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[10,128,19,30,40] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/sequential/max_pooling3d/MaxPool3D/MaxPool3DGrad (defined at <ipython-input-17-43dbc76b3aa7>:27) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_11835]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "if model_charge:\n",
    "    print('Il ne faut pas train le modele car il a déjà été chargé')\n",
    "else:\n",
    "    print('Il faut entrainer le modele')\n",
    "    start = time()\n",
    "    model, histories, _,_ = homemade_fit_gen(model, labels, pack_size = pack_size, epochs = epochs,batch_size = batch_size, fps = fps, size = size)\n",
    "    end = time()\n",
    "    print(f\"ça a prit {end-start} secondes d'entrainer le modele\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-appraisal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-suggestion",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, _ = my_gen(labels_tests, \"DATA\\\\V_tests\",pack_size = 1000,fps = fps, size = size)\n",
    "#Grand batch_size afin de prendre toutes les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-olympus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-dining",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "asian-treasure",
   "metadata": {},
   "source": [
    "### visualisation des  résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fidle.pwk as pwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_arg = np.argmax(y_pred, axis=-1)\n",
    "plot_confusion_name = full_name + '_confusion.png'\n",
    "pwk.plot_confusion_matrix(y_test,y_pred_arg,range(nb_classes),normalize=True,save_as = plot_confusion_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Je peux afficher l'historique de chaque étape (je n'ai aps réussi à les faire fusionner)\n",
    "#On ne peut afficher que si on a train\n",
    "\n",
    "#history = histories[-1]\n",
    "#pwk.plot_history(history, figsize=(6,4), save_as='03-history')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-lodge",
   "metadata": {},
   "source": [
    "## Enregistrer le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le nom est fabriqué en début de notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM, 5 FPS, Height = 60, Width = 80, nb_Classes = 10, 3 Epochs, Batch_size = 10, Pack_size = 50\n",
    "if not model_charge: #On ne doit charger le modele que si il a été train cette fois-ci\n",
    "    model.save(full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-complex",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-masters",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-potter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-valuable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-logan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-salon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-single",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-modeling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
