
\documentclass[a4paper, 10pt, conference]{IEEEtran}
\usepackage{float}
\usepackage{cite}
\usepackage{subcaption}
\usepackage{hyperref} % Pour faire des liens dans le texte vers des section
% \FGfinalcopy % *** Uncomment this line for the final submission


%\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
%\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed

%\addtolength{\textheight}{-3cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.
                                  


\title{Real Time Hand Gesture Recognition in Industry
}

\author{\IEEEauthorblockN{William Dumoulin}
\IEEEauthorblockA{\textit{High School of Industrial Engineering of Henallux Pierrard} \\
%\textit{name of organization (of Aff.)}\\
Virton, Belgium \\
etu37765@henallux.be}
\and
\IEEEauthorblockN{Nicolas Thiry}
\IEEEauthorblockA{\textit{High School of Industrial Engineering of Henallux Pierrard} \\
%\textit{name of organization (of Aff.)}\\
Virton, Belgium \\
etu38083@henallux.be}
\and
\IEEEauthorblockN{Rim Slama}
\IEEEauthorblockA{\textit{High School of Industrial Engineering of Henallux Pierrard} \\
%\textit{name of organization (of Aff.)}\\
Virton, Belgium \\
rim.slama@henallux.be}
\and

\IEEEauthorblockN{Michel Bernard}
\IEEEauthorblockA{\textit{High School of Industrial Engineering of Henallux Pierrard} \\
%\textit{name of organization (of Aff.)}\\
Virton, Belgium \\
michel.bernard@henallux.be}
}

%\maketitle

\usepackage{graphicx}
\graphicspath{ {./images/} }

\pagestyle{plain} % Ajoute un numéro en bas de page

\begin{document}



\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
With the 4th industrial revolution and the increased use of cobots in the industries comes many opportunities for new generation control panels. In this article, we proposed  to develop a deep learning model to recognise in real time 10 different gestures that can be used to interact with a cobot. We proposed a new dataset containing gestures that can be used in industrial context. The videos were taken from a computer webcam and then processed to remove the noise created by the background by isolating the movement of the gray scale images. We proposed to extract the spatio-temporal features by the combination of 3D convolution and LSTM layers. We also proposed a real time method to recognize our gestures. Our experimental results show  for 8 out of 10 gestures, a recognition rate of more than 90\%. Furthermore, an interface was created to test our method in real time and to add a new class of gestures to be recognized by our model.
\end{abstract}
\begin{IEEEkeywords}
Deep Learning, Hand gesture recognition, Human Robot Interaction, Real time recognition
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

In more recent years, the world of industry has witnessed a major change, referred to as "Industry 4.0". In fact, robots are now used in crowded environments and in shared areas with operators, the thing that demands a focus on the development of communication techniques with robots like gesture recognition for human robot interaction.
There are different levels of human robot interaction according to its degree: coexistence, synchronization, cooperation and collaboration.  Collaboration is the highest level of interaction among these levels where robot and operator share the same space and may work simultaneously on the same component. In this context, we need a specific algorithm able to recognize gestures needed to control a cobot without a control panel.

Existing human gesture datasets mostly contain gestures for sign language recognition or virtual reality context with a clear lack of industrial hand gesture recognition datasets and methods.

This paper proposed to bring a solution in this context. Indeed, we propose an interface that we used to acquire a new dataset with specific gestures that can be used in industrial context.  The collected dataset is then used within  a convolutional approach to extract spatio-temporal features in order to classify gestures. Moreover, we proposed a real time approach to recognize gestures using a friendly interface. The interface mainly shows a feed-back of the camera with the predictions from the model. There is also the possibility to create new videos and add new gestures in order to enrich the dataset with desired gestures.


The rest of the paper is organized as follows: Section II
examines state of the art methods for gesture recognition
And used datasets. Section III introduce
the proposed dataset that we created in industrial context. In Section IV, we will explain
Proposed approach for gesture recognition. Section V
presents the experimental results. Section VI explains how to recognize gestures in real time. Finally, the last
section concludes the paper with future research directions.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{STATE-OF-THE-ART}

Many different techniques were developed in the past years for hand gesture recognition in general. One of the first was an approach that separates the fingers from the palm \cite{chen2014real}. The position of the hand on the static image was then deduced depending on the number of fingers  found by the algorithm and their relative position.
A much more developed method is to apply a skeletonization method \cite{ogniewicz1992voronoi}, \cite{ionescu2005dynamic}. The idea is to reduce an object to lines that are only one pixel wide. The images are therefore easier to analyse by the model. This method is applied to the videos before the training and the tests. An algorithm to create the skeleton of a shape need a Voronoi tessellation \cite{beristain2009skeleton} but it's a heavy algorithm that slows down the entire process. A solution would be to use the champfer distance transform \cite{butt1998optimum} to reduce the computational cost.

N. Luthfil Hakim, T. K. Shih et al \cite{hakim2019dynamic} created a model of gesture recognition for a television application. They used an RGB and a depth camera to get more valuable information. Their model is composed by multiple blocs of 3D convolution layers combined with dense and max pooling layers. The last part of their model is the Long Short-Term Memory layers \cite{hochreiter1997long} that finds features that are time related which are crucial when dealing with videos.

Another basic method is the haar-like features \cite{chen2007real}  that are used for face or eyes detections. This method is called a hand crafted feature extraction method. If the color changes are not sharp enough, this method will show no results.
The LSTM layer is an advanced type of RNN \cite{giles1994dynamic}. Enhanced methods of Recurrent Neural Network that could be used for gesture recognition were developed : Differencial RNN \cite{veeriah2015differential}, Hierarchical RNN \cite{du2015hierarchical}, Bidirectionnal RNN \cite{pigou2018beyond}.

Some articles\cite{hakim2019dynamic}, \cite{ren2011robust} use the kinect camera to get the depth information in addition to the 2D images.


Transformers \cite{vaswani2017attention}, \cite{de2020sign} are a type of layers that is designed to handle sequential data. Unlike LSTMs, they use a mechanism to give different weighs to the input data.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed dataset}
\label{sec:methods}
\subsection{Dataset creation protocol}

In order to recognize gestures in industrial context and for cobot controlling with gestures, we decided to create a new dataset. In order to collect this dataset, we created a program with a friendly interface that enables us to easily register videos with hand gestures.
The program is made to create a lot of data in the smallest amount of time. The user can switch between different labels by pressing a key and can start a new video with another key. See details on Figure  \ref{fig:new_video_program}. There is a delay of one second before the video is taken to give the time to the user to place himself. The computer's camera has a frame rate of 25 FPS so each video lasts 2.4 seconds and contains 61 frames. With a good rhythm, a new video can be made every 5 seconds.

A dataset needs recordings of different people otherwise the model would only recognise the gesture of the person that did all of the data set videos.
The smallest data set needs at least 12 different people and each one does each gesture twice.
This dataset has 10 different gestures which make a minimum of 240 videos for the training set.
The result is a training set of almost 1 500 videos.

The data set is not split into a training and a test group. The data set in used in full for the training. Before the subjects recorded their movement, they were told to do the gesture the way that they wanted, with no other specification then the name of the movement and the time that they had to do it. However when we reviewed the videos, it appeared that they were all very similar regarding the speed and position of the gesture in the image. We decided to keep all of these videos in the training set and to create ourselves the test set to have more challenging videos. The gestures are done slower or faster than the training set and in various places on the screen. 

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{images/Interface_new_video.png}
    \caption{New video program}
    \label{fig:new_video_program}
\end{figure}


\subsection{Data processing}

The videos from the data set have 61 frames with a dimension of 640 pixels wide by 480 pixels height and 3 layers of depths for the red, green and blue channels.
First step, conversion from RGB to gray image: Each pixel of the new gray image is calculated with the colors channels of source image with this formula:
\(Gray \leftarrow 0.299 * Red + 0.587 * Green + 0.114 * Blue.\)

The second step is to isolate the movement in the images. To do that, each image is compared with the next one. The movement of the last frame can't be isolated because there are no more frame to do the comparison. That make 60 frames of movement.
The pros of keeping only the movement is to remove the background. Of course, to make the training data, the background need to be still, otherwise it will be considered as part of the gesture. This technique still reduces the noise that can be produced by the background.

The model doesn't get 60 frames for each video because it would make an over complicated model. The third step is to isolate a sample of the frames. The choice was made to take only 10 fps, that make a sample of 24 frames.

The fourth step is to resize and normalize the videos. The normalization take all the pixels and map them from 0-255 to 0-1.
After all the processing, the videos have 24 frames with a dimension of 120 pixels wide by 90 pixels height. All the pixels have a value between 0 and 1. See figure \ref{fig:normalization} for comparison.

The data set is available on the github page :             .The data is alrealdy processed like previously explained.
\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width = \textwidth]{images/normalisation_avant.JPG}
         \caption{Before}
         \label{fig:before}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width =\textwidth]{images/normalisation_après.JPG}
         \caption{After}
         \label{fig:after}
     \end{subfigure}
     \hfill
     \caption{Normalization}
     \label{fig:normalization}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed approach}


In order to recognize gestures on our collected dataset, we first proposed to test the model based on the work of N. L. Hakim, T. K. Shih, et al\cite{hakim2019dynamic}. In this article, data are collected from an RGB and a depth camera. The images from the two cameras are processed separately and combined after the LSTM units \cite{ullah2017action}. The image processing is made with multiple 3D convolution layers.
The authors proposed to use a data set with thousands of videos and got a result of 91\%.
Our setup does not include a depth camera. The RGB images are therefore processed on their own. 
As expected, testing this approach on our data was not concluding. The problem was because of the small data set we are using and the large amount of parameters proposed in this model. Thus, we simplified it to create another model more efficient in this context.
We trained 15 different models, each one had a small difference compare to the last one in order to understand how each parameter affects the model.
Its accuracy raised from 26\% to 83\%. A clear improvement in the quality of the models can be seen by the increase of their accuracy : the 4th model showed an accuracy of 46\% while the 12th showed an accuracy of 79\%.

Our final model that we selected is composed of 3 blocs: The first two are an arrangement of Convolution 3d, dense, dropout, maxpooling 3d and batch normalization. The last one has two ConvLSTM2D followed by a long-short term memory and a dense layer to have as many outputs as we have classes.
\\
\subsubsection{3D Convolution}
In the convolution layers  \cite{ji20123d}, \cite{tran2015learning}, new channels are created with different kernels. A kernel is a squared matrix, whose size is generally 3x3 or 5x5.
To analyse videos, we chose to use 3D convolution layers, with a size of 3x3x3 or 5x5x5.
The goal of this layers is to find the features of the images in the video.

\subsubsection{Dense}
The dense layer is also called fully-connected layer. Each node of a layer is connected to all the nodes of the next layer. In our case a node is an entire image. Each channel of the last dimension is also an image. For example, the dimensions of the first dense layer are (24x90x120x256) which stands for 24 moments in time, 90 pixels height by 120 pixels wide and 256 images. The different images for one moment in time are created by the convolution layer.

The connection is defined by a weight with a value between minus one and one. In this architecture, the dense layer is placed after a convolution one. We can compare the dense layer to a filter. The meaningful channels created in the 3D convolution layer have a weight with a high value, the useless ones have a weight close to zero.

\subsubsection{Dropout}
When this layer is placed after a dense layer, it removes a certain percentage of the weights by giving them new random values. This layer is added in order to reduce the risk of overfitting.

\subsubsection{Max pooling 3D}
This layer reduces the size of the data set by keeping the max value of a section of an image (or multiple images when it's 3D). A 3D Max pooling layer with a matrix (3x3x3): The dimension go from (24,90,120,256) to (24,30,40,85) because the three last dimensions are divided by 3. This layer simplifies the data by keeping only the meaningful pixels.

\subsubsection{Batch Normalization}
The layer transforms inputs so that they are standardized. This means that they have a mean value of zero and a standard deviation of one.

\subsubsection{LSTM }
Long Short-Term Memory \cite{hochreiter1997long} layers are a type of recurrent neural network capable of finding time dependent features.
This layer (and the ConvLSTM) is very important in this application because there are needed to differentiate "Swipe to the left" and "Swipe to the right" for example.


\subsubsection{ConvLSTM2D}
Like the classic LSTM, the convLSTM is capable of learning order dependence in sequence but this layer is specialized for the videos analyses because it's the combination of LSTM and convolution layer.

In this architecture, two ConvLSTM 2D are stacked.
The first ConvLSTM layer provides an output for each input while the second layer gives only one output for a sequence of input.

\subsubsection{Reshape}
The layer doesn't learn anything in the training process. It only changes the dimension of the data to pass them to the LSTM layer which needs specific input.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{EXPERIMENTS RESULTS}

The videos are stored in a raw way, they need to be processed when they are loaded to train the model. The data set of 1 500 videos is too heavy to be loaded in a standard computer's RAM, so the videos have to be load by package.
The label of the videos are saved in a list which is randomly mixed. The videos can then be loaded in the order of the list. It's important that the model is trained on different gestures every time.

To train the model faster, the data processing and training are on two different threads. The process thread loads two packs of 40 videos and waits for the training part to be done with the previous pack.
When the model is done training with a pack, the next pack is transferred to the training section and the thread loads the next one.

The 2 tasks are handled by the computer in parallel, that's what is called thread. It allows to reduce the overall time of computing.
The videos are loaded by packs to avoid overloading the computer's memory.
The packs have enough information to allow multiples training. However, if a model is trained multiple times on the same pack, the risk of overfitting increases.

To reduce this risk, the model should be trained on the entire data set once before being trained on videos that it already saw.

The ideal process would be to train the model on the entire data set in a single pack. Since it is not possible due to memory size, the videos should be loaded by smaller packs. Each one of them should then train the model once and leave its place to the next pack. If multiple epochs are mandatory, the whole process needs to start again.

Multiple epochs can be necessary when the model has a bad accuracy.

Our algorithm doesn't use that process because each videos would be processed multiple times and the training would be unnecessarily long. Our model is trained on each pack three times in a row before moving to the next pack.
\begin{figure}
    \centering
    %\includegraphics[scale = 0.7]{Confusion_Matrix_Normalized.png}
    \includegraphics[width = 0.5\textwidth]{Confusion_Matrix_Normalized.jpg}
    \caption{Confusion matrix}
    \label{fig:confusion}
\end{figure}
\subsection{Accuracy}

The Figure \ref{fig:confusion} shows the confusion matrix of our best model.
The numbers on the columns and lines of the confusion matrix correspond to this list in table  \ref{tab:gestures}\\


\begin{table} [H]
\caption{List of gestures}
\label{tab:gestures}
\begin{center}
\begin{tabular}{|c|c|}
\hline
0 & Closing one hand\\
\hline
1 & Opening one hand\\
\hline
2 & Clap with both hands\\
\hline
3 & The right hand swipes to the left\\
\hline
4 & The left hand swipes to the right\\
\hline
5 & Crossed fingers\\
\hline
6 & The hands start closed in the middle of the image and they go\\
& in opposite directions while they open\\
\hline
7 & The right arm starts horizontally and rotates around the elbow,\\
& it ends when the hand is pointing to the sky\\
\hline
8 & The left arm starts horizontally and rotates around the elbow,\\
& it ends when the hand is pointing to the sky\\
\hline
9 & Waving\\
\hline
\end{tabular}
\end{center}
\end{table}


The first two gestures are often confused with one another. This is probably due to the proximity of theses gesture. We can see that closing a hand and opening a hand is done at the same places on the videos. Furthermore, the difference between closing and opening a hand is much more subtle than the difference between swipe left and swipe right.

For all the others gestures, the model is very efficient. On the live test, we often got a precision of +90\%.









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Real time recognition application}

\subsection{Real time recognition method}
The application works with threads, there is one for the prediction and another one for the interface. The videos are recorded at 10 frames per seconds, which makes a pack of 24 frames, and then are sent to the prediction. 24 frames is the number of images that were given per video to train the model.The frames are processed directly when they are taken, the list of processed images is sent to the prediction thread.
A CPU Intel i7-1065G7 takes 1.5s while a GPU Nvidia gtx1050 takes 0.4s to get the prediction done.
More predictions could be done in parallel. For example send a video for prediction every 12 frames. The 12 other frames would be kept from the end of the previous video. However it could create some lag.


\subsection{Interface}

The interface was coded using the python libraries tkinter and openCV.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{images/Fig4.png}
    \caption{First Window}
    \label{fig:window1}
\end{figure}
The main window, that can be seen at the Figure \ref{fig:window1}, shows the live feed from the camera. It also shows the prediction that have the highest score at the bottom of the screen. Furthermore the window shows the 3 first predictions for the same video with the percentage of recognition. If the first prediction given by the model is lower than 60\%, the predictions are not displayed and a message is written on the screen explaining that the model did not recognise the gesture. \\
The last information that is shown is the number of frames that are already captured in the video. This allows the person in front of the camera to perform the full gesture on one video. As soon as the video is fully captured, another video is being captured. \\
The main window also shows two buttons. The first one freezes the prediction. In other words, the videos are no longer captured and the prediction stays on the window. The second button opens a second window. While the second window is active, the first is completely freezed to avoid wasting the resources of the computer.


\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{images/Fig5.png}
    \caption{Second window}
    \label{fig:window2}
\end{figure}
The second window, that can be seen at the Figure \ref{fig:window2}, displays an example of each gesture that the model is able to recognise with its name. It also allows the user to add a new gesture. The first step is to create a new label or title for the gesture and add it to the list through the text field on the window. If the label already exists on the list, a message is displayed on the screen. \\
On this window, there are two buttons. The first one opens program to record a video and add it to the training set. This program is explained at \nameref{sec:methods}: A. Data set. The second button retrains the model. This operation takes a lot of time. Therefore, a safety needs to be installed to avoid miss-clicks.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CONCLUSIONS AND FUTURE WORKS}
In this paper, we proposed a dataset collected within a new friendly used interface in order to acquire hand gestures. The collected data aims to be used for hand gesture recognition for controlling cobots in industrial context. We also proposed a well adapted architecture of a deep learning approach that is able to recognize in real time such gestures with efficiency.
 The result we obtained allows us to obtain an overall accuracy of 84\% and 90\% for 8 gestures out of 10.
In future works, we intend to  collect more data in the industrial context using our developed interface. We also have to focus on resolving the problem of gesture confusion by improving the model performances.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{IEEEtran}
\bibliography{main}

\end{document}



